{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXME :: Takes very long to execute. Or should buy graphics card eventually.\n",
    "\n",
    "TRIALS=500\n",
    "MAXSTEPS=1000\n",
    "\n",
    "DIM = 4\n",
    "\n",
    "ALPHA=0.1       # Learnrate\n",
    "EPSILON=0.01    # Randomness over Exploration versus Exploitation\n",
    "GAMMA=0.9       # Influence of single training examples\n",
    "\n",
    "T.seed = 1337\n",
    "np.random.seed(T.seed)\n",
    "random.seed(T.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self, d=4, mode='static'):\n",
    "        if d < 4:\n",
    "            raise Exception(\"Dimension should be equal to 4.\")\n",
    "        if mode not in ['static', 'player-dynamic', 'dynamic']:\n",
    "            raise Exception(\"The mode should be either static, player-dynamic or dynamic.\")\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.d = d\n",
    "        self.maxMoves = 25\n",
    "\n",
    "        self.actionSpace = {'up': (-1, 0), 'right': (0, 1), 'down': (1, 0), 'left': (0, -1)}               \n",
    "        self.cellTypes = [' ', '+', '-', 'P'] \n",
    "        self.cellTypesMap = {' ': 0, '+': 1, '-': 2, 'P': 3}       \n",
    "        self.rewardMap = {' ': -1, '+': 25, '-': -10}\n",
    "        \n",
    "        # Initialize the environment\n",
    "        self.reset()\n",
    "\n",
    "    # Transform the grid to a multi-dimensional array, to later flatten it into 1-dimension vector for torch\n",
    "    def transformGrid(self):\n",
    "        grid = []\n",
    "        cellTypes = ['+', '-', 'P']\n",
    "        for a in cellTypes:\n",
    "            plane = np.zeros_like(self.grid, dtype=np.int8)\n",
    "            for b in cellTypes:\n",
    "                if a == b:\n",
    "                    plane = np.where(self.grid == b, 1, plane)\n",
    "            grid.append(plane)\n",
    "        return np.array(grid)\n",
    "\n",
    "    # Reset to initial state\n",
    "    def reset(self):\n",
    "        self.moveCount = 0\n",
    "        self.grid = np.tile(np.array(self.cellTypes[0]), (self.d, self.d))\n",
    "        \n",
    "        self.treasure = np.array([0, 0])\n",
    "        self.pitfall = np.array([0, 3])\n",
    "        self.player = np.array([3, 2])\n",
    "            \n",
    "        # If mode is player, set the player position to random\n",
    "        if self.mode == 'player-dynamic':\n",
    "            self.player = np.random.choice(self.d, size=2)            \n",
    "\n",
    "        if self.mode == 'dynamic':\n",
    "            self.pitfall = np.random.choice(self.d, size=2)  \n",
    "            self.treasure = np.random.choice(self.d, size=2)  \n",
    "            self.player = np.random.choice(self.d, size=2)  \n",
    "            \n",
    "        while (self.treasure == self.player).all():\n",
    "            self.treasure = np.random.choice(self.d, size=2)\n",
    "\n",
    "        while (self.pitfall == self.player).all() or (self.pitfall == self.treasure).all():\n",
    "            self.pitfall = np.random.choice(self.d, size=2)\n",
    "            \n",
    "\n",
    "        self.grid[self.treasure[0], self.treasure[1]] = self.cellTypes[1] # Win\n",
    "        self.grid[self.pitfall[0], self.pitfall[1]] = self.cellTypes[2] # Loss\n",
    "        self.grid[self.player[0], self.player[1]] = self.cellTypes[3] # Player\n",
    "\n",
    "        return self.transformGrid().flatten()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.moveCount += 1\n",
    "        reward = -1\n",
    "        info = ''\n",
    "        done = False\n",
    "\n",
    "        # Setup new coordinates\n",
    "        x, y = copy.copy(self.player) + self.actionSpace[action]\n",
    "        if self.moveCount < self.maxMoves:\n",
    "            if not done:                                    \n",
    "                # Check that we are not at the edges\n",
    "                if x >= 0 and x < self.d and y >= 0 and y < self.d:\n",
    "                    reward = self.rewardMap[self.grid[x, y]]\n",
    "                        # Check if its game won or over\n",
    "                    if self.grid[x, y] == '+' or self.grid[x, y] == '-':\n",
    "                        done = True\n",
    "                        # Update the grid with this new transition                             \n",
    "                    self.grid[x, y] = 'P'\n",
    "                    self.grid[self.player[0], self.player[1]] = ' '\n",
    "                        \n",
    "                    # Update the player position\n",
    "                    self.player = np.array([x, y])\n",
    "\n",
    "        # Return the next state, reward received, is done flag and any other info                \n",
    "        return self.transformGrid().flatten(), reward, done, info\n",
    "\n",
    "    # Renders the game to the screen\n",
    "    def render(self):\n",
    "        print(self.grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchQAgent():\n",
    "    def __init__(self, envDimension=4, learningRate=1e-3, gamma=0.9, maxEpsilon = 1, minEpsilon=0.1):\n",
    "        self.envDimension = envDimension\n",
    "        self.learningRate = learningRate\n",
    "        self.gamma = gamma\n",
    "        self.maxEpsilon = maxEpsilon\n",
    "        self.minEpsilon = minEpsilon\n",
    "\n",
    "        self.availableActions = list(GridWorld(self.envDimension, mode='static').actionSpace.keys())\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "        linQLayerIn = self.envDimension * 4 * 3\n",
    "        linqLayerOut = len(self.availableActions)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(linQLayerIn, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, 144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(144, linqLayerOut)\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.learningRate)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def train(self, mode='static', epochs=50):                 \n",
    "        model = self.model \n",
    "        optimizer = self.optimizer\n",
    "        lossFn = self.loss\n",
    "\n",
    "        epsilon = self.maxEpsilon\n",
    "        rewards = []\n",
    "\n",
    "        for epoch in range(epochs):            \n",
    "            env = GridWorld(self.envDimension, mode=mode)\n",
    "            state = T.from_numpy(env.reset()).to(self.device).float()\n",
    "            reward_per_epoch = []\n",
    "            done = False            \n",
    "            while not done:\n",
    "                Q = model(state)\n",
    "                Q_ = Q.data.cpu().numpy()\n",
    "\n",
    "                if(np.random.random() < epsilon):\n",
    "                    action = np.random.choice(len(self.availableActions))\n",
    "                else:                                     \n",
    "                    action = np.argmax(Q_)\n",
    "\n",
    "                nextState_, reward, done, _ = env.step(self.availableActions[action])\n",
    "                nextState = T.from_numpy(nextState_).to(self.device).float()\n",
    "                reward_per_epoch.append(reward)\n",
    "\n",
    "                state = nextState\n",
    "                nextQ = model(nextState)                        \n",
    "                maxNextQ = T.max(nextQ)                    \n",
    "\n",
    "                if not done:\n",
    "                    YHat = reward + (self.gamma * maxNextQ)\n",
    "                else:\n",
    "                    YHat = reward\n",
    "                    rewards.append(np.sum(reward_per_epoch))\n",
    "                    \n",
    "                YHat = T.Tensor([YHat]).detach().to(self.device)                                       \n",
    "                Y = Q.squeeze()[action]            \n",
    "                                \n",
    "                loss = lossFn(Y, YHat)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()              \n",
    "\n",
    "            # Adapt the epsilon value\n",
    "            if epsilon > self.minEpsilon:\n",
    "                epsilon -= 0.01        \n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Processing epoch {}\".format(epoch))\n",
    "\n",
    "        plt.title('TorchQAgent Training - Static Gridworld (A:{}, G:{}, E:{})'.format(ALPHA, GAMMA, EPSILON))\n",
    "        plt.xlabel(\"Trials\")\n",
    "        plt.ylabel(\"Scores\")\n",
    "        plt.plot(rewards)\n",
    "\n",
    "        picname = 'TorchQAgent_{}x{}_Static_Alp{}_Gam{}_Eps{}.png'.format(DIM, DIM, ALPHA,GAMMA, EPSILON)\n",
    "        plt.savefig(picname)\n",
    "\n",
    "    def test(self, numGames=10, mode='static', display=False):\n",
    "        numWins = 0\n",
    "        numLoss = 0\n",
    "\n",
    "        for game in range(numGames):        \n",
    "            testEnv = GridWorld(4, mode=mode)        \n",
    "            state = T.from_numpy(testEnv.reset()).to(self.device).float()\n",
    "            m = 0        \n",
    "            done = False\n",
    "            maxMoves = 10\n",
    "            while not done and maxMoves > 0:\n",
    "                maxMoves -= 1\n",
    "\n",
    "                Q = self.model(state)\n",
    "                Q_ = Q.data.cpu().numpy()                        \n",
    "\n",
    "                action = np.argmax(Q_)    \n",
    "                action = self.availableActions[action]\n",
    "                \n",
    "                nextState_, reward, done, _ = testEnv.step(action)    \n",
    "                state = T.from_numpy(nextState_).to(self.device).float()\n",
    "                \n",
    "                m += 1         \n",
    "                print('Game {}, Step {}, Action {}, Reward {}'.format(game, m, action, reward))\n",
    "                if display:\n",
    "                    testEnv.render()\n",
    "                \n",
    "                if done:                \n",
    "                    if reward == 25:\n",
    "                        numWins += 1\n",
    "                    if reward == -10:\n",
    "                        numLoss += 1\n",
    "            \n",
    "        print('Total Games={}. Wins={}, Losses={}'.format(numGames, numWins, numLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TorchQAgent(learningRate=ALPHA, gamma=GAMMA, maxEpsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on static mode\n",
    "agent.test(numGames=50, mode='static', display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on player mode\n",
    "agent.test(numGames=50, mode='player-dynamic', display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on random mode\n",
    "agent.test(numGames=50, mode='dynamic', display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
